{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AceR00kIe/trading_view_cookie/blob/main/worker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package"
      ],
      "metadata": {
        "id": "rf6xJFCaC7UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_client = False # if job == 'BTs' and part == '0' else False"
      ],
      "metadata": {
        "id": "iV4vmt58C_yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Co-operate_Training/'\n",
        "\n",
        "# Install dependencies\n",
        "\n",
        "try :\n",
        "    import nest_asyncio\n",
        "    import os\n",
        "    from distributed import Scheduler, Client, Worker, Nanny\n",
        "    from pyngrok import ngrok\n",
        "    from pinggy import Tunnel\n",
        "    import asyncio\n",
        "    import re\n",
        "except :\n",
        "    !pip install dask distributed pyngrok nest_asyncio pinggy --quiet\n",
        "\n",
        "    import nest_asyncio\n",
        "    import os\n",
        "    from distributed import Scheduler, Client, Worker, Nanny\n",
        "    from pyngrok import ngrok\n",
        "    from pinggy import Tunnel\n",
        "    import asyncio\n",
        "    import re\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def async_wait_for_workers(client, required_workers=2, timeout=180, poll_interval=1):\n",
        "    start = asyncio.get_event_loop().time()\n",
        "    while True:\n",
        "        info = client.scheduler_info()\n",
        "        n_workers = len(info['workers'])\n",
        "        if n_workers >= required_workers:\n",
        "            print(f\"{n_workers} workers connected.\")\n",
        "            break\n",
        "        if asyncio.get_event_loop().time() - start > timeout:\n",
        "            raise TimeoutError(f\"Timeout waiting for {required_workers} workers.\")\n",
        "        print(f\"Waiting for workers... currently {n_workers} / {required_workers} connected.\")\n",
        "        await asyncio.sleep(poll_interval)\n",
        "\n",
        "\n",
        "async def start_scheduler(port, public_url):\n",
        "    scheduler = await Scheduler(host=\"0.0.0.0\", port=port, dashboard_address=\"0.0.0.0:8787\")\n",
        "    print(f\"Scheduler started at {public_url} | {scheduler.address}\")\n",
        "\n",
        "    # Save to Google Drive file\n",
        "    scheduler_address_path = f'{file_path}colab_aws/dask_scheduler_address.txt'\n",
        "    ## Ensure file is exist\n",
        "    os.makedirs(os.path.dirname(scheduler_address_path), exist_ok=True)\n",
        "    with open(scheduler_address_path, 'w') as f:\n",
        "        f.write(public_url)\n",
        "    print(f\"Scheduler address saved to {scheduler_address_path}\")\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "def task() :\n",
        "    pass\n",
        "\n",
        "def close_worker_sync(dask_worker):\n",
        "    asyncio.run(dask_worker.close())\n",
        "\n",
        "async def run_client(scheduler_address, scheduler):\n",
        "    # Connect client to scheduler\n",
        "    client = await Client(scheduler_address, asynchronous=True)\n",
        "    print(client)\n",
        "    print(client.scheduler_info())\n",
        "    print(f\"Client connected to scheduler at {scheduler_address}\")\n",
        "\n",
        "    # Wait for more worker incoming\n",
        "    print(\"Waiting worker\")\n",
        "    await async_wait_for_workers(client)#, required_workers=2)\n",
        "    #await client.wait_for_workers(n_workers=2)\n",
        "\n",
        "    # Define some Dask-distributed tasks\n",
        "    def square(x):\n",
        "        print(f\"Worker is computing square({x})\")\n",
        "        return x ** 2\n",
        "\n",
        "    # Submit tasks to cluster asynchronously\n",
        "    print(\"Client Sending tasks\")\n",
        "    futures = client.map(square, range(20))\n",
        "\n",
        "    # Gather and print results when ready\n",
        "    print(\"Client Gathering results\")\n",
        "    results = await client.gather(futures, direct=False) # asynchronous=True,\n",
        "    print(\"Results from distributed computation:\", results)\n",
        "\n",
        "    # Wait for worker task to finish (if started as asyncio task)\n",
        "    #await worker_task\n",
        "    #await client.run(lambda dask_worker: dask_worker.close())\n",
        "    #await client.run(close_worker_sync)\n",
        "    #print(\"Client closing worker\")\n",
        "\n",
        "    # Close client connection\n",
        "    #await client.close()\n",
        "    #print(\"Client closed\")\n",
        "\n",
        "    # Shutdown scheduler\n",
        "    #await scheduler.close()\n",
        "    #print(\"Scheduler closed\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\"\"\"\n",
        "async def start_worker(scheduler_address):\n",
        "    worker = await Worker(scheduler_address, nthreads=2, memory_limit=\"4GB\")\n",
        "    print(f\"Worker connected to scheduler at {scheduler_address}\")\n",
        "    await worker.finished()\n",
        "\"\"\"\n",
        "\n",
        "async def start_pinggy(port=9001):\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        \"ssh\",\n",
        "        \"-o\", \"StrictHostKeyChecking=no\",\n",
        "        \"-p\", \"443\",\n",
        "        f\"-R0:localhost:{port}\",\n",
        "        \"tcp@a.pinggy.io\", ## we need TCP port\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.STDOUT\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        line = await process.stdout.readline()\n",
        "        if not line:\n",
        "            return None\n",
        "        decoded = line.decode().strip()\n",
        "        print(decoded)\n",
        "        #match = re.findall(r'tcp://[a-zA-Z0-9\\-\\.]+\\.free.pinggy\\.', decoded)\n",
        "        match = re.findall(r'tcp://[a-zA-Z0-9\\-\\.]+\\.pinggy\\.link:\\d+', decoded)\n",
        "        if match:\n",
        "            #public_url = match.group(1)\n",
        "            public_url = match[-1] ## <--- this should be tcp://.......\n",
        "            print(f\"Pinggy (localhost:{port}) tunnel opened at: {public_url}\")\n",
        "            #break\n",
        "            return public_url\n",
        "\n",
        "async def start_worker(scheduler_address, worker_address=None, worker_port=None):\n",
        "    # Run the async function\n",
        "    if worker_address == None and worker_port :\n",
        "        ## using pinggy to get public url\n",
        "        worker_address = await start_pinggy(worker_port)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            nthreads     = 2\n",
        "            memory_limit = \"10GB\"\n",
        "            if worker_address and worker_port :\n",
        "                #worker = await Worker(scheduler_address, host=\"0.0.0.0\", contact_address=worker_address, port=worker_port) #nthreads=nthreads, memory_limit=memory_limit,\n",
        "                worker = await Nanny(scheduler_address, host=\"0.0.0.0\", contact_address=worker_address, port=worker_port, name=worker_address) #nthreads=nthreads, memory_limit=memory_limit,\n",
        "                print(f\"Worker binding to 0.0.0.0:{worker_port}, public address: {worker_address}\")\n",
        "            else :\n",
        "                #worker = await Worker(scheduler_address, host=\"0.0.0.0\") #, nthreads=nthreads, memory_limit=memory_limit\n",
        "                worker = await Nanny(scheduler_address, host=\"0.0.0.0\") #, nthreads=nthreads, memory_limit=memory_limit\n",
        "            print(f\"Worker connected to scheduler at {scheduler_address}\")\n",
        "            await worker.finished()  # blocks here until worker shuts down\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Worker failed with {e}, restarting in 5 seconds...\")\n",
        "            await asyncio.sleep(5)  # retry after delay\n",
        "\n",
        "\n",
        "try : ## Disconnect if exist\n",
        "    ngrok.disconnect(public_tunnel)\n",
        "except :\n",
        "    pass\n",
        "\n",
        "if is_client : ## Client Part + Scheduler + Worker\n",
        "\n",
        "    # === ngrok authtoken here ===\n",
        "    NGROK_AUTH_TOKEN = \"2z1TC2xKjmC8LYOYg77KHFZX5Cm_5NRMuVVdn7fN9cJkuCLTZ\"  # Replace this with your real token\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "    SCHEDULER_PORT = 8786 ## Dask port\n",
        "    DASHBOARD_PORT = 8787\n",
        "\n",
        "    # Open ngrok TCP tunnel to expose scheduler port publicly\n",
        "    public_tunnel    = ngrok.connect(SCHEDULER_PORT, \"tcp\")\n",
        "    dashboard_tunnel = ngrok.connect(DASHBOARD_PORT, \"http\")\n",
        "\n",
        "    SCHEDULER_ADDRESS = public_tunnel.public_url\n",
        "    DASHBOARD_ADDRESS = dashboard_tunnel.public_url\n",
        "\n",
        "    print(f\"Ngrok public URL (for workers and client): {SCHEDULER_ADDRESS}\")\n",
        "    print(f\"Dashboard ngrok TCP URL: {DASHBOARD_ADDRESS}\")\n",
        "\n",
        "    async def main():\n",
        "        print(\"Starting Scheduler\")\n",
        "        # Start scheduler (just like server channel for communication)\n",
        "        scheduler = await start_scheduler(SCHEDULER_PORT, SCHEDULER_ADDRESS)\n",
        "        print(\"Scheduler running\")\n",
        "\n",
        "        # wait the scheduler completed\n",
        "        await asyncio.sleep(2)\n",
        "\n",
        "        # be part of worker after client assign job\n",
        "        #await start_worker(SCHEDULER_ADDRESS) ## This will blocking\n",
        "\n",
        "        # Start worker(s) concurrently (does not block)\n",
        "        print(\"Starting Worker\")\n",
        "        worker_task = asyncio.create_task(start_worker(SCHEDULER_ADDRESS))\n",
        "        print(\"Worker running\")\n",
        "        # wait the scheduler completed\n",
        "        await asyncio.sleep(2)\n",
        "\n",
        "        # Run client jobs after scheduler started\n",
        "        #await run_client(SCHEDULER_ADDRESS, scheduler)\n",
        "\n",
        "        # Run client jobs (client is async)\n",
        "        print(\"Starting Client\")\n",
        "        results = await run_client(SCHEDULER_ADDRESS, scheduler)\n",
        "\n",
        "        # Keep scheduler alive so workers can connect\n",
        "        await scheduler.finished()\n",
        "        print(\"schedule finished\")\n",
        "\n",
        "        # Delete the previous scheduler address path\n",
        "        if os.path.exists(scheduler_address_path):\n",
        "            os.remove(scheduler_address_path)\n",
        "            print(f\"Deleted file: {scheduler_address_path}\")\n",
        "        else:\n",
        "            print(\"File does not exist.\")\n",
        "\n",
        "        # Shutdown workers gracefully\n",
        "        await client.run(lambda dask_worker: dask_worker.close())\n",
        "\n",
        "        # Close client connection\n",
        "        await client.close()\n",
        "\n",
        "    # Run the combined scheduler + client event loop\n",
        "    asyncio.get_event_loop().run_until_complete(main())\n",
        "\n",
        "else : ## Worker Part\n",
        "\n",
        "    \"\"\"\n",
        "    ## Ngrok version\n",
        "    NGROK_AUTH_TOKEN = \"2z68NN59CTemkEVI5hg6Qr9FQDa_251TVBwa47obznQZ22JY4\"\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    WORKER_PORT      = 9001 ## Dask port\n",
        "    worker_tunnel    = ngrok.connect(WORKER_PORT, \"tcp\")\n",
        "    WORKER_ADDRESS   = worker_tunnel.public_url\n",
        "    print(f\"worker tunnel opened at: {WORKER_ADDRESS}\")\n",
        "    \"\"\"\n",
        "    WORKER_PORT      = 9001 ## Dask port\n",
        "\n",
        "    # Load from Google Drive file\n",
        "    scheduler_address_path = f'{file_path}colab_aws/dask_scheduler_address.txt'\n",
        "    # Read the scheduler address\n",
        "    if os.path.exists(scheduler_address_path):\n",
        "        with open(scheduler_address_path, 'r') as f:\n",
        "            SCHEDULER_ADDRESS = f.read().strip()\n",
        "    else :\n",
        "        print(\"scheduler_address_path : Not found\")\n",
        "\n",
        "    print(\"Scheduler address read from file:\", SCHEDULER_ADDRESS)\n",
        "    #SCHEDULER_ADDRESS = \"tcp://0.tcp.ngrok.io:XXXXX\"  # Replace with your ngrok URL\n",
        "\n",
        "    async def main() :\n",
        "        print(\"Starting Worker\")\n",
        "        #worker_task = asyncio.create_task(start_worker(SCHEDULER_ADDRESS, WORKER_ADDRESS, WORKER_PORT))\n",
        "        worker_task = asyncio.create_task(start_worker(SCHEDULER_ADDRESS, worker_port=WORKER_PORT))\n",
        "        print(\"Worker running\")\n",
        "        await worker_task\n",
        "\n",
        "    # Run the worker\n",
        "    asyncio.get_event_loop().run_until_complete(main())\n",
        "\n",
        "    try : ## Disconnect if exist\n",
        "        #ngrok.disconnect(worker_tunnel)\n",
        "        worker_tunnel.stop()\n",
        "    except :\n",
        "        pass\n",
        "\n"
      ],
      "metadata": {
        "id": "rPX1qU3_DNmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77025014-5b2f-4230-93b8-a14920706211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hScheduler address read from file: tcp://8.tcp.ngrok.io:14764\n",
            "Starting Worker\n",
            "Worker running\n",
            "Pseudo-terminal will not be allocated because stdin is not a terminal.\n",
            "Warning: Permanently added '[a.pinggy.io]:443' (RSA) to the list of known hosts.\n",
            "Allocated port 3 for remote forward to localhost:9001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.worker:      Start worker at: tcp://rnvpa-35-237-96-255.a.free.pinggy.link:42593\n",
            "INFO:distributed.worker:         Listening to: tcp://rnvpa-35-237-96-255.a.free.pinggy.link:42593\n",
            "INFO:distributed.worker:         dashboard at: rnvpa-35-237-96-255.a.free.pinggy.link:45459\n",
            "INFO:distributed.worker:Waiting to connect to: tcp://8.tcp.ngrok.io:14764\n",
            "INFO:distributed.worker:-------------------------------------------------\n",
            "INFO:distributed.worker:              Threads:                          2\n",
            "INFO:distributed.worker:               Memory:                  12.67 GiB\n",
            "INFO:distributed.worker:      Local Directory: /tmp/dask-scratch-space/worker-e7oy868r\n",
            "INFO:distributed.worker:-------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are not authenticated.\n",
            "Your tunnel will expire in 60 minutes. Upgrade to Pinggy Pro to get unrestricted tunnels. https://dashboard.pinggy.io\n",
            "tcp://rnvpa-35-237-96-255.a.free.pinggy.link:42593\n",
            "Pinggy (localhost:9001) tunnel opened at: tcp://rnvpa-35-237-96-255.a.free.pinggy.link:42593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.worker:Starting Worker plugin shuffle\n",
            "INFO:distributed.worker:        Registered to: tcp://8.tcp.ngrok.io:14764\n",
            "INFO:distributed.worker:-------------------------------------------------\n",
            "INFO:distributed.core:Starting established connection to tcp://8.tcp.ngrok.io:14764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker binding to 0.0.0.0:9001, public address: tcp://rnvpa-35-237-96-255.a.free.pinggy.link:42593\n",
            "Worker connected to scheduler at tcp://8.tcp.ngrok.io:14764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.worker:Run out-of-band function 'set'\n",
            "INFO:distributed.worker:Run out-of-band function 'set'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}